{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8c37242-2cdb-4374-b1f4-58e01d5b05a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='6'\n",
    "\n",
    "import gradio as gr\n",
    "from types import SimpleNamespace\n",
    "import torch\n",
    "from transformers import pipeline, AutoTokenizer, GPT2LMHeadModel\n",
    "from langdetect import detect\n",
    "from torch.nn import functional as F\n",
    "from itertools import chain\n",
    "import logging\n",
    "import tensorflow as tf\n",
    "\n",
    "# Set TensorFlow logging level to ERROR\n",
    "logging.getLogger('tensorflow').setLevel(logging.ERROR)\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import argparse\n",
    "import random\n",
    "\n",
    "\n",
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "#######################################################################################################\n",
    "class Inferencer():\n",
    "    def __init__(self, args):\n",
    "        self.args = args\n",
    "        if self.args.language == 'ko':\n",
    "            #print(\"Loading Korean Chatbot...\")\n",
    "            self.args.model_path = self.args.model_path_ko\n",
    "        elif self.args.language == 'en':\n",
    "            #print(\"Loading English Chatbot...\")\n",
    "            self.args.model_path = self.args.model_path_en\n",
    "        else:\n",
    "            print(\"Not supported!\")\n",
    "        \n",
    "        # Tokenizer & Vocab\n",
    "        #print(\"Loading the tokenizer...\")\n",
    "\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.args.model_path)\n",
    "        special_tokens = self.tokenizer.special_tokens_map\n",
    "        self.args.bos_token = special_tokens['bos_token']\n",
    "        self.args.eos_token = special_tokens['eos_token']\n",
    "        self.args.sp1_token = special_tokens['additional_special_tokens'][0]\n",
    "        self.args.sp2_token = special_tokens['additional_special_tokens'][1]\n",
    "\n",
    "        vocab = self.tokenizer.get_vocab()\n",
    "        self.vocab_size = len(vocab)\n",
    "        self.args.bos_id = vocab[self.args.bos_token]\n",
    "        self.args.eos_id = vocab[self.args.eos_token]\n",
    "        self.args.sp1_id = vocab[self.args.sp1_token]\n",
    "        self.args.sp2_id = vocab[self.args.sp2_token]\n",
    "        \n",
    "        # Load model    \n",
    "        #print(\"Loading the model...\")\n",
    "        self.fix_seed(0)\n",
    "        self.model = GPT2LMHeadModel.from_pretrained(self.args.model_path).to(device)\n",
    "        self.args.max_len = self.model.config.n_ctx\n",
    "        self.model.resize_token_embeddings(len(self.tokenizer))\n",
    "              \n",
    "        #print(\"Setting finished.\")\n",
    "              \n",
    "    def infer(self, input, history):        \n",
    "        self.model.eval()\n",
    "        self.fix_seed(0)\n",
    "        with torch.no_grad():         \n",
    "\n",
    "            input_hists=[]\n",
    "\n",
    "            for line in history:\n",
    "                input_hists.append([self.args.sp1_id] + self.tokenizer.encode(line[0]))\n",
    "                input_hists.append([self.args.sp2_id] + self.tokenizer.encode(line[1]))\n",
    "            \n",
    "            utter = input\n",
    "            input_ids = [self.args.sp1_id] + self.tokenizer.encode(utter)\n",
    "            input_hists.append(input_ids)                \n",
    "            input_ids = [self.args.bos_id] + list(chain.from_iterable(input_hists)) + [self.args.sp2_id]\n",
    "            start_sp_id = input_hists[0][0]\n",
    "            next_sp_id = self.args.sp1_id if start_sp_id == self.args.sp2_id else self.args.sp2_id\n",
    "            assert start_sp_id != next_sp_id\n",
    "            token_type_ids = [[start_sp_id] * len(hist) if h % 2 == 0 else [next_sp_id] * len(hist) for h, hist in enumerate(input_hists)]\n",
    "            assert len(token_type_ids) == len(input_hists)\n",
    "            token_type_ids = [start_sp_id] + list(chain.from_iterable(token_type_ids)) + [self.args.sp2_id]\n",
    "            assert len(input_ids) == len(token_type_ids)\n",
    "            input_len = len(input_ids)\n",
    "            \n",
    "            input_ids = torch.LongTensor(input_ids).unsqueeze(0).to(device)\n",
    "            token_type_ids = torch.LongTensor(token_type_ids).unsqueeze(0).to(device)\n",
    "            \n",
    "            output_ids = self.nucleus_sampling(input_ids, token_type_ids, input_len)                \n",
    "            res = self.tokenizer.decode(output_ids, skip_special_tokens=True)\n",
    "            \n",
    "            return res\n",
    "                \n",
    "    def nucleus_sampling(self, input_ids, token_type_ids, input_len):\n",
    "        output_ids = []\n",
    "        for pos in range(input_len, self.args.max_len):\n",
    "            output = self.model(input_ids=input_ids, token_type_ids=token_type_ids)[0][:, pos-1]  # (1, V)\n",
    "            output = F.softmax(output, dim=-1)  # (1, V)\n",
    "            \n",
    "            sorted_probs, sorted_idxs = torch.sort(output, descending=True)\n",
    "            cumsum_probs = torch.cumsum(sorted_probs, dim=-1)  # (1, V)\n",
    "            idx_remove = cumsum_probs > self.args.top_p\n",
    "            idx_remove[:, 1:] = idx_remove[:, :-1].clone()\n",
    "            idx_remove[:, 0] = False\n",
    "            sorted_probs[idx_remove] = 0.0\n",
    "            sorted_probs /= torch.sum(sorted_probs, dim=-1, keepdim=True)  # (1, V)\n",
    "            \n",
    "            probs = torch.zeros(output.shape, device=device).scatter_(-1, sorted_idxs, sorted_probs)  # (1, V)\n",
    "            idx = torch.multinomial(probs, 1)  # (1, 1)\n",
    "            \n",
    "            idx_item = idx.squeeze(-1).squeeze(-1).item()\n",
    "            output_ids.append(idx_item)\n",
    "            \n",
    "            if idx_item == self.args.eos_id:\n",
    "                break\n",
    "                \n",
    "            input_ids = torch.cat((input_ids, idx), dim=-1)\n",
    "            next_type_id = torch.LongTensor([[self.args.sp2_id]]).to(device)\n",
    "            token_type_ids = torch.cat((token_type_ids, next_type_id), dim=-1)\n",
    "            assert input_ids.shape == token_type_ids.shape\n",
    "            \n",
    "        return output_ids\n",
    "    \n",
    "    def fix_seed(self, seed):\n",
    "        np.random.seed(seed)\n",
    "        torch.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        random.seed(seed)\n",
    "\n",
    "\n",
    "\n",
    "args = SimpleNamespace(\n",
    "    seed = 0,\n",
    "    language = 'ko', \n",
    "    model_path_ko = \"path/to/korean/chat/model\",\n",
    "    model_path_en = \"path/to/english/chat/model\",\n",
    "    top_p=0.8)\n",
    "\n",
    "inferencer_kaist_ko=Inferencer(args)\n",
    "\n",
    "args = SimpleNamespace(\n",
    "    seed = 0,\n",
    "    language = 'en', \n",
    "    model_path_ko = \"path/to/korean/chat/model\",\n",
    "    model_path_en = \"path/to/english/chat/model\",\n",
    "    top_p=0.8)\n",
    "\n",
    "inferencer_kaist_en=Inferencer(args)\n",
    "\n",
    "\n",
    "\n",
    "def chat_kaist_en(message, input_hists):\n",
    "    max_turns = 4\n",
    "    \n",
    "    if len(input_hists) >= max_turns:\n",
    "        num_exceeded = len(input_hists) - max_turns + 1\n",
    "        input_hists = input_hists[num_exceeded:]\n",
    "        \n",
    "    reply = inferencer_kaist_en.infer(message, input_hists)\n",
    "    return reply\n",
    "\n",
    "\n",
    "\n",
    "def chat_kaist_ko(message, input_hists):\n",
    "    max_turns = 4\n",
    "    \n",
    "    if len(input_hists) >= max_turns:\n",
    "        num_exceeded = len(input_hists) - max_turns + 1\n",
    "        input_hists = input_hists[num_exceeded:]\n",
    "        \n",
    "    reply = inferencer_kaist_ko.infer(message, input_hists)\n",
    "    return reply\n",
    "\n",
    "\n",
    "\n",
    "#######################################################################################################\n",
    "\n",
    "theme = gr.themes.Base(\n",
    "    primary_hue=\"indigo\",\n",
    "    secondary_hue=gr.themes.Color(c100=\"#ffffff\", c200=\"#ffffff\", c300=\"#d4d4d4\", c400=\"#a3a3a3\", c50=\"#ffffff\", c500=\"#737373\", c600=\"#525252\", c700=\"#404040\", c800=\"#262626\", c900=\"#171717\", c950=\"#0f0f0f\"),\n",
    "    neutral_hue=\"slate\",\n",
    ").set(\n",
    "    body_text_color='*neutral_950',\n",
    "    body_text_color_subdued='*neutral_900'\n",
    ")\n",
    "\n",
    "#######################################################################################################\n",
    "\n",
    "kaistchat_en_interface = gr.ChatInterface(\n",
    "    fn=chat_kaist_en, \n",
    "    examples=[\"Hello!\", \"What is KI Building?\", \"When was KAIST established?\"], \n",
    "    title=\"KAIST Chatbot (English)\",\n",
    "    theme=theme\n",
    ")\n",
    "\n",
    "\n",
    "kaistchat_ko_interface = gr.ChatInterface(\n",
    "    fn=chat_kaist_ko, \n",
    "    examples=[\"안녕!\", \"KI 빌딩에 대해서 알려줘.\", \"KAIST는 언제 설립됐어?\"], \n",
    "    title=\"KAIST Chatbot (Korean)\",\n",
    "    theme=theme\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "#######################################################################################################\n",
    "\n",
    "app = gr.TabbedInterface(\n",
    "    [kaistchat_en_interface, kaistchat_ko_interface],\n",
    "    ['English','Korean'],\n",
    "    theme=theme\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "543cc018-ac4a-4c01-b46f-0cedb78f1957",
   "metadata": {},
   "outputs": [],
   "source": [
    "app.launch(share=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ecff32-937a-44d5-aeef-3ac801ff76b3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
