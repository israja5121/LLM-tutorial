{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d8c37242-2cdb-4374-b1f4-58e01d5b05a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sslunder24/env/transformer/lib/python3.8/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/sslunder24/env/transformer/lib/python3.8/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n",
      "2024-08-26 03:24:56.672712: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-08-26 03:24:57.378444: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='6'\n",
    "\n",
    "import gradio as gr\n",
    "from types import SimpleNamespace\n",
    "import torch\n",
    "from transformers import pipeline, AutoTokenizer, GPT2LMHeadModel\n",
    "from langdetect import detect\n",
    "from torch.nn import functional as F\n",
    "from itertools import chain\n",
    "import logging\n",
    "import tensorflow as tf\n",
    "\n",
    "# Set TensorFlow logging level to ERROR\n",
    "logging.getLogger('tensorflow').setLevel(logging.ERROR)\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import argparse\n",
    "import random\n",
    "\n",
    "\n",
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "#######################################################################################################\n",
    "class Inferencer():\n",
    "    def __init__(self, args):\n",
    "        self.args = args\n",
    "        if self.args.language == 'ko':\n",
    "            #print(\"Loading Korean Chatbot...\")\n",
    "            self.args.model_path = self.args.model_path_ko\n",
    "        elif self.args.language == 'en':\n",
    "            #print(\"Loading English Chatbot...\")\n",
    "            self.args.model_path = self.args.model_path_en\n",
    "        else:\n",
    "            print(\"Not supported!\")\n",
    "        \n",
    "        # Tokenizer & Vocab\n",
    "        #print(\"Loading the tokenizer...\")\n",
    "\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.args.model_path)\n",
    "        special_tokens = self.tokenizer.special_tokens_map\n",
    "        self.args.bos_token = special_tokens['bos_token']\n",
    "        self.args.eos_token = special_tokens['eos_token']\n",
    "        self.args.sp1_token = special_tokens['additional_special_tokens'][0]\n",
    "        self.args.sp2_token = special_tokens['additional_special_tokens'][1]\n",
    "\n",
    "        vocab = self.tokenizer.get_vocab()\n",
    "        self.vocab_size = len(vocab)\n",
    "        self.args.bos_id = vocab[self.args.bos_token]\n",
    "        self.args.eos_id = vocab[self.args.eos_token]\n",
    "        self.args.sp1_id = vocab[self.args.sp1_token]\n",
    "        self.args.sp2_id = vocab[self.args.sp2_token]\n",
    "        \n",
    "        # Load model    \n",
    "        #print(\"Loading the model...\")\n",
    "        self.fix_seed(0)\n",
    "        self.model = GPT2LMHeadModel.from_pretrained(self.args.model_path).to(device)\n",
    "        self.args.max_len = self.model.config.n_ctx\n",
    "        self.model.resize_token_embeddings(len(self.tokenizer))\n",
    "              \n",
    "        #print(\"Setting finished.\")\n",
    "              \n",
    "    def infer(self, input, history):        \n",
    "        self.model.eval()\n",
    "        self.fix_seed(0)\n",
    "        with torch.no_grad():         \n",
    "\n",
    "            input_hists=[]\n",
    "\n",
    "            for line in history:\n",
    "                input_hists.append([self.args.sp1_id] + self.tokenizer.encode(line[0]))\n",
    "                input_hists.append([self.args.sp2_id] + self.tokenizer.encode(line[1]))\n",
    "            \n",
    "            utter = input\n",
    "            input_ids = [self.args.sp1_id] + self.tokenizer.encode(utter)\n",
    "            input_hists.append(input_ids)                \n",
    "            input_ids = [self.args.bos_id] + list(chain.from_iterable(input_hists)) + [self.args.sp2_id]\n",
    "            start_sp_id = input_hists[0][0]\n",
    "            next_sp_id = self.args.sp1_id if start_sp_id == self.args.sp2_id else self.args.sp2_id\n",
    "            assert start_sp_id != next_sp_id\n",
    "            token_type_ids = [[start_sp_id] * len(hist) if h % 2 == 0 else [next_sp_id] * len(hist) for h, hist in enumerate(input_hists)]\n",
    "            assert len(token_type_ids) == len(input_hists)\n",
    "            token_type_ids = [start_sp_id] + list(chain.from_iterable(token_type_ids)) + [self.args.sp2_id]\n",
    "            assert len(input_ids) == len(token_type_ids)\n",
    "            input_len = len(input_ids)\n",
    "            \n",
    "            input_ids = torch.LongTensor(input_ids).unsqueeze(0).to(device)\n",
    "            token_type_ids = torch.LongTensor(token_type_ids).unsqueeze(0).to(device)\n",
    "            \n",
    "            output_ids = self.nucleus_sampling(input_ids, token_type_ids, input_len)                \n",
    "            res = self.tokenizer.decode(output_ids, skip_special_tokens=True)\n",
    "            \n",
    "            return res\n",
    "                \n",
    "    def nucleus_sampling(self, input_ids, token_type_ids, input_len):\n",
    "        output_ids = []\n",
    "        for pos in range(input_len, self.args.max_len):\n",
    "            output = self.model(input_ids=input_ids, token_type_ids=token_type_ids)[0][:, pos-1]  # (1, V)\n",
    "            output = F.softmax(output, dim=-1)  # (1, V)\n",
    "            \n",
    "            sorted_probs, sorted_idxs = torch.sort(output, descending=True)\n",
    "            cumsum_probs = torch.cumsum(sorted_probs, dim=-1)  # (1, V)\n",
    "            idx_remove = cumsum_probs > self.args.top_p\n",
    "            idx_remove[:, 1:] = idx_remove[:, :-1].clone()\n",
    "            idx_remove[:, 0] = False\n",
    "            sorted_probs[idx_remove] = 0.0\n",
    "            sorted_probs /= torch.sum(sorted_probs, dim=-1, keepdim=True)  # (1, V)\n",
    "            \n",
    "            probs = torch.zeros(output.shape, device=device).scatter_(-1, sorted_idxs, sorted_probs)  # (1, V)\n",
    "            idx = torch.multinomial(probs, 1)  # (1, 1)\n",
    "            \n",
    "            idx_item = idx.squeeze(-1).squeeze(-1).item()\n",
    "            output_ids.append(idx_item)\n",
    "            \n",
    "            if idx_item == self.args.eos_id:\n",
    "                break\n",
    "                \n",
    "            input_ids = torch.cat((input_ids, idx), dim=-1)\n",
    "            next_type_id = torch.LongTensor([[self.args.sp2_id]]).to(device)\n",
    "            token_type_ids = torch.cat((token_type_ids, next_type_id), dim=-1)\n",
    "            assert input_ids.shape == token_type_ids.shape\n",
    "            \n",
    "        return output_ids\n",
    "    \n",
    "    def fix_seed(self, seed):\n",
    "        np.random.seed(seed)\n",
    "        torch.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        random.seed(seed)\n",
    "\n",
    "\n",
    "\n",
    "args = SimpleNamespace(\n",
    "    seed = 0,\n",
    "    language = 'ko', \n",
    "    model_path_ko = \"path/to/korean/chat/model\",\n",
    "    model_path_en = \"path/to/english/chat/model\",\n",
    "    top_p=0.8)\n",
    "\n",
    "inferencer_kaist_ko=Inferencer(args)\n",
    "\n",
    "args = SimpleNamespace(\n",
    "    seed = 0,\n",
    "    language = 'en', \n",
    "    model_path_ko = \"path/to/korean/chat/model\",\n",
    "    model_path_en = \"path/to/english/chat/model\",\n",
    "    top_p=0.8)\n",
    "\n",
    "inferencer_kaist_en=Inferencer(args)\n",
    "\n",
    "\n",
    "\n",
    "def chat_kaist_en(message, input_hists):\n",
    "    max_turns = 4\n",
    "    \n",
    "    if len(input_hists) >= max_turns:\n",
    "        num_exceeded = len(input_hists) - max_turns + 1\n",
    "        input_hists = input_hists[num_exceeded:]\n",
    "        \n",
    "    reply = inferencer_kaist_en.infer(message, input_hists)\n",
    "    return reply\n",
    "\n",
    "\n",
    "\n",
    "def chat_kaist_ko(message, input_hists):\n",
    "    max_turns = 4\n",
    "    \n",
    "    if len(input_hists) >= max_turns:\n",
    "        num_exceeded = len(input_hists) - max_turns + 1\n",
    "        input_hists = input_hists[num_exceeded:]\n",
    "        \n",
    "    reply = inferencer_kaist_ko.infer(message, input_hists)\n",
    "    return reply\n",
    "\n",
    "\n",
    "\n",
    "#######################################################################################################\n",
    "\n",
    "theme = gr.themes.Base(\n",
    "    primary_hue=\"indigo\",\n",
    "    secondary_hue=gr.themes.Color(c100=\"#ffffff\", c200=\"#ffffff\", c300=\"#d4d4d4\", c400=\"#a3a3a3\", c50=\"#ffffff\", c500=\"#737373\", c600=\"#525252\", c700=\"#404040\", c800=\"#262626\", c900=\"#171717\", c950=\"#0f0f0f\"),\n",
    "    neutral_hue=\"slate\",\n",
    ").set(\n",
    "    body_text_color='*neutral_950',\n",
    "    body_text_color_subdued='*neutral_900'\n",
    ")\n",
    "\n",
    "#######################################################################################################\n",
    "\n",
    "kaistchat_en_interface = gr.ChatInterface(\n",
    "    fn=chat_kaist_en, \n",
    "    examples=[\"Hello!\", \"What is KI Building?\", \"When was KAIST established?\"], \n",
    "    title=\"KAIST Chatbot (English)\",\n",
    "    theme=theme\n",
    ")\n",
    "\n",
    "\n",
    "kaistchat_ko_interface = gr.ChatInterface(\n",
    "    fn=chat_kaist_ko, \n",
    "    examples=[\"안녕!\", \"KI 빌딩에 대해서 알려줘.\", \"KAIST는 언제 설립됐어?\"], \n",
    "    title=\"KAIST Chatbot (Korean)\",\n",
    "    theme=theme\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "#######################################################################################################\n",
    "\n",
    "app = gr.TabbedInterface(\n",
    "    [kaistchat_en_interface, kaistchat_ko_interface],\n",
    "    ['English','Korean'],\n",
    "    theme=theme\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "543cc018-ac4a-4c01-b46f-0cedb78f1957",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "Running on public URL: https://eedd95dbd4089aae2b.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://eedd95dbd4089aae2b.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/sslunder24/env/transformer/lib/python3.8/site-packages/gradio/queueing.py\", line 536, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "  File \"/home/sslunder24/env/transformer/lib/python3.8/site-packages/gradio/route_utils.py\", line 285, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "  File \"/home/sslunder24/env/transformer/lib/python3.8/site-packages/gradio/blocks.py\", line 1923, in process_api\n",
      "    result = await self.call_function(\n",
      "  File \"/home/sslunder24/env/transformer/lib/python3.8/site-packages/gradio/blocks.py\", line 1506, in call_function\n",
      "    prediction = await fn(*processed_input)\n",
      "  File \"/home/sslunder24/env/transformer/lib/python3.8/site-packages/gradio/utils.py\", line 785, in async_wrapper\n",
      "    response = await f(*args, **kwargs)\n",
      "  File \"/home/sslunder24/env/transformer/lib/python3.8/site-packages/gradio/chat_interface.py\", line 607, in _submit_fn\n",
      "    response = await anyio.to_thread.run_sync(\n",
      "  File \"/home/sslunder24/env/transformer/lib/python3.8/site-packages/anyio/to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "  File \"/home/sslunder24/env/transformer/lib/python3.8/site-packages/anyio/_backends/_asyncio.py\", line 2177, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "  File \"/home/sslunder24/env/transformer/lib/python3.8/site-packages/anyio/_backends/_asyncio.py\", line 859, in run\n",
      "    result = context.run(func, *args)\n",
      "  File \"/tmp/ipykernel_57695/1917714908.py\", line 172, in chat_kaist_ko\n",
      "    reply = inferencer_kaist_ko.infer(message, input_hists)\n",
      "  File \"/tmp/ipykernel_57695/1917714908.py\", line 75, in infer\n",
      "    input_ids = [self.args.sp1_id] + self.tokenizer.encode(utter)\n",
      "  File \"/home/sslunder24/env/transformer/lib/python3.8/site-packages/transformers/tokenization_utils_base.py\", line 2825, in encode\n",
      "    encoded_inputs = self.encode_plus(\n",
      "  File \"/home/sslunder24/env/transformer/lib/python3.8/site-packages/transformers/tokenization_utils_base.py\", line 3237, in encode_plus\n",
      "    return self._encode_plus(\n",
      "  File \"/home/sslunder24/env/transformer/lib/python3.8/site-packages/transformers/models/gpt2/tokenization_gpt2_fast.py\", line 137, in _encode_plus\n",
      "    return super()._encode_plus(*args, **kwargs)\n",
      "  File \"/home/sslunder24/env/transformer/lib/python3.8/site-packages/transformers/tokenization_utils_fast.py\", line 601, in _encode_plus\n",
      "    batched_output = self._batch_encode_plus(\n",
      "  File \"/home/sslunder24/env/transformer/lib/python3.8/site-packages/transformers/models/gpt2/tokenization_gpt2_fast.py\", line 127, in _batch_encode_plus\n",
      "    return super()._batch_encode_plus(*args, **kwargs)\n",
      "  File \"/home/sslunder24/env/transformer/lib/python3.8/site-packages/transformers/tokenization_utils_fast.py\", line 528, in _batch_encode_plus\n",
      "    encodings = self._tokenizer.encode_batch(\n",
      "TypeError: TextEncodeInput must be Union[TextInputSequence, Tuple[InputSequence, InputSequence]]\n"
     ]
    }
   ],
   "source": [
    "app.launch(share=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ecff32-937a-44d5-aeef-3ac801ff76b3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
