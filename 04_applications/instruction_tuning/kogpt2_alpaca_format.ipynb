{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d4e79c0-9f35-451b-b46a-1b2f72b5126f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -U accelerate peft bitsandbytes transformers trl datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd8371d5-22d1-463c-8dfd-f88cd4989b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"4,\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7e032669-702c-4e36-9f06-52ecb3e63eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    TrainingArguments,\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    ")\n",
    "from trl import SFTTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9eecd4b3-b3af-48fe-93e0-43d234e2a260",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dir = \"./saved_models/kogpt2_koalpaca\"\n",
    "model_name = \"skt/kogpt2-base-v2\"\n",
    "batch_size = 8\n",
    "num_train_epochs = 5\n",
    "logging_steps = 500\n",
    "bf16 = False\n",
    "fp16 = True\n",
    "context_length = 256\n",
    "num_workers = 1\n",
    "# num_workers = os.cpu_count()\n",
    "gradient_accumulation_steps = 2\n",
    "learning_rate = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2acc21e1-0c5f-44f2-9753-68d593b85750",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "dataset = load_dataset('bingsu/ko_alpaca_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a0c8df6a-0dcf-40b9-b684-dfbd65f4fc2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['instruction', 'input', 'output'],\n",
      "        num_rows: 49620\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7c7ab862-dc3d-42c5-a8bb-9fa98de60542",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "건강을 유지하기 위한 세 가지 팁을 알려주세요.\n"
     ]
    }
   ],
   "source": [
    "print(dataset['train']['instruction'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "43f5d886-b436-417a-93c4-5920eb237204",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'instruction': '건강을 유지하기 위한 세 가지 팁을 알려주세요.', 'input': '', 'output': '세 가지 팁은 아침식사를 꼭 챙기며, 충분한 수면을 취하고, 적극적으로 운동을 하는 것입니다.'}\n"
     ]
    }
   ],
   "source": [
    "print(dataset['train'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3a3fddc5-281a-44d2-8250-efadafa5dfd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set size: 47139, Valid set size: 2481\n"
     ]
    }
   ],
   "source": [
    "# Create train set and validation set\n",
    "full_dataset = dataset['train'].train_test_split(test_size=0.05, shuffle=True)\n",
    "dataset_train = full_dataset['train']\n",
    "dataset_valid = full_dataset['test']\n",
    "\n",
    "print(f\"Train set size: {len(dataset_train)}, Valid set size: {len(dataset_valid)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d641edc8-90ef-4c89-9b6f-10deb8a5d7cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'instruction': '균형 잡힌 식단의 다섯 가지 구성 요소를 말하십시오.', 'input': '', 'output': '균형 잡힌 식단에는 곡류, 채소, 단백질, 유지 및 과일이 포함됩니다.'}\n",
      "****************\n",
      "### Instruction:\n",
      "균형 잡힌 식단의 다섯 가지 구성 요소를 말하십시오.\n",
      "\n",
      "### Input:\n",
      "\n",
      "\n",
      "### Response:\n",
      "균형 잡힌 식단에는 곡류, 채소, 단백질, 유지 및 과일이 포함됩니다.\n",
      "##################################################\n",
      "{'instruction': '웹사이트의 신뢰성 여부를 판단하는 알고리즘을 만드세요.', 'input': '', 'output': '웹사이트 신뢰성 판단의 일반적인 방법 중 하나는 해당 웹사이트의 SSL/TLS 인증서 여부입니다. SSL/TLS 인증서가 있는 경우, 정보가 암호화되고 안전한 연결이 보장됩니다. 또는 해당 웹사이트가 보유한 개인 정보, 역사, 평판 및 기타 정보를 확인하여 신뢰할 수 있는지 여부를 결정할 수도 있습니다.'}\n",
      "****************\n",
      "### Instruction:\n",
      "웹사이트의 신뢰성 여부를 판단하는 알고리즘을 만드세요.\n",
      "\n",
      "### Input:\n",
      "\n",
      "\n",
      "### Response:\n",
      "웹사이트 신뢰성 판단의 일반적인 방법 중 하나는 해당 웹사이트의 SSL/TLS 인증서 여부입니다. SSL/TLS 인증서가 있는 경우, 정보가 암호화되고 안전한 연결이 보장됩니다. 또는 해당 웹사이트가 보유한 개인 정보, 역사, 평판 및 기타 정보를 확인하여 신뢰할 수 있는지 여부를 결정할 수도 있습니다.\n",
      "##################################################\n",
      "{'instruction': '선택한 캐릭터에 대한 짧은 이야기를 작성합니다.', 'input': '캐릭터: 여우 아델', 'output': '옛날 옛적에 여우 아델이 울길 많은 날이었습니다.'}\n",
      "****************\n",
      "### Instruction:\n",
      "선택한 캐릭터에 대한 짧은 이야기를 작성합니다.\n",
      "\n",
      "### Input:\n",
      "캐릭터: 여우 아델\n",
      "\n",
      "### Response:\n",
      "옛날 옛적에 여우 아델이 울길 많은 날이었습니다.\n",
      "##################################################\n"
     ]
    }
   ],
   "source": [
    "# Check dataset as alpaca prompt format\n",
    "for i in range(3):\n",
    "    print(dataset_train[i])\n",
    "    print('****************')\n",
    "    \n",
    "    text = dataset_train[i]\n",
    "    instruction = '### Instruction:\\n' + text['instruction']\n",
    "    inputs = '\\n\\n### Input:\\n' + text['input']\n",
    "    response = '\\n\\n### Response:\\n' + text['output']\n",
    "    \n",
    "    final_text = instruction + inputs + response\n",
    "    print(final_text)\n",
    "    print('#'*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "47976b50-8ea5-4ea1-b6cc-7f30596e39a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping function for dataset\n",
    "def preprocess_function(examples):\n",
    "    \"\"\"\n",
    "    Formatting function returning a list of processed strings.\n",
    "    \"\"\"\n",
    "    texts = []\n",
    "    \n",
    "    for example in zip(examples['instruction'], examples['input'], examples['output']):\n",
    "        instruction, input_text, output = example\n",
    "        text = f\"### 지시:\\n{instruction}\\n\\n### 자료:\\n{input_text}\\n\\n### 응답:\\n{output}\"\n",
    "        texts.append(text)\n",
    "    \n",
    "    return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9fc364a6-e5c3-4576-94ef-b5451e4a6abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "if bf16:\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name).to(dtype=torch.bfloat16)\n",
    "else:\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7ddaeb80-f3ec-44be-8dfb-902acab4d9f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT2LMHeadModel(\n",
      "  (transformer): GPT2Model(\n",
      "    (wte): Embedding(51200, 768)\n",
      "    (wpe): Embedding(1024, 768)\n",
      "    (drop): Dropout(p=0.1, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-11): 12 x GPT2Block(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPT2SdpaAttention(\n",
      "          (c_attn): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPT2MLP(\n",
      "          (c_fc): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=51200, bias=False)\n",
      ")\n",
      "125,164,032 total parameters.\n",
      "125,164,032 training parameters.\n"
     ]
    }
   ],
   "source": [
    "# Check model\n",
    "print(model)\n",
    "# Total parameters and trainable parameters.\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"{total_params:,} total parameters.\")\n",
    "total_trainable_params = sum(\n",
    "    p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"{total_trainable_params:,} training parameters.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2bd6ebdb-38d8-484d-b6f6-07b26f0778a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name, \n",
    "    trust_remote_code=True,\n",
    "    use_fast=True,\n",
    ")\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.add_special_tokens({'pad_token': '</s>', 'eos_token':'</s>'})\n",
    "    model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "60b7632b-a2fe-4582-a000-430f1367c665",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=f\"{out_dir}/logs\",\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    eval_strategy='epoch',\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    logging_strategy='steps',\n",
    "    logging_steps=logging_steps,\n",
    "    save_strategy='epoch',\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    "    bf16=bf16,\n",
    "    fp16=fp16,\n",
    "    weight_decay=0.01,\n",
    "    report_to='tensorboard',\n",
    "    dataloader_num_workers=num_workers,\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    learning_rate=learning_rate,\n",
    "    lr_scheduler_type='constant',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "df5e5b09-66e8-410e-8e83-d9d6facbe19b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sslunder13/env/transformers/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.\n",
      "\n",
      "Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "/home/sslunder13/env/transformers/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:280: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "488c06a67bfe425c80465d2c73d1467f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/47139 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b5a25f9eb2d427aadb1a1261f3f519a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2481 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 4.15.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "# Load trainer\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=dataset_train,\n",
    "    eval_dataset=dataset_valid,\n",
    "    max_seq_length=context_length,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_args,\n",
    "    formatting_func=preprocess_function,\n",
    "    # packing=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "09c7209d-f67b-4958-b673-7f1bfafe3d0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='14730' max='14730' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [14730/14730 31:11, Epoch 4/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>2.401500</td>\n",
       "      <td>2.309197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.883500</td>\n",
       "      <td>2.287354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.507000</td>\n",
       "      <td>2.403161</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "There were missing keys in the checkpoint model loaded: ['lm_head.weight'].\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "history = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d533dac5-30ed-4dd8-afe9-8447ccd592d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./saved_models/kogpt2_koalpaca/tokenizer_config.json',\n",
       " './saved_models/kogpt2_koalpaca/special_tokens_map.json',\n",
       " './saved_models/kogpt2_koalpaca/vocab.json',\n",
       " './saved_models/kogpt2_koalpaca/merges.txt',\n",
       " './saved_models/kogpt2_koalpaca/added_tokens.json',\n",
       " './saved_models/kogpt2_koalpaca/tokenizer.json')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save model\n",
    "model.save_pretrained(out_dir)\n",
    "tokenizer.save_pretrained(out_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d68cd3eb-1e5d-445f-bfdd-914acb50f3e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    AutoModelForCausalLM, \n",
    "    logging, \n",
    "    pipeline,\n",
    "    AutoTokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "74eaa9ea-e781-484b-9f13-0dbaaa0bf88a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load newly trained model\n",
    "model = AutoModelForCausalLM.from_pretrained(\"/home/sslunder13/project/06_instruction_tuning/saved_models/kogpt2_koalpaca/kogpt2_koalpaca\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"/home/sslunder13/project/06_instruction_tuning/saved_models/kogpt2_koalpaca/kogpt2_koalpaca\")\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c19190e1-eb73-480e-bf97-73aef6150ead",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    }
   ],
   "source": [
    "# Inference using fine-tuned model\n",
    "pipe = pipeline(task='text-generation', model=model, tokenizer=tokenizer, max_length=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "da4db689-9dfa-4367-bceb-a90fc9d4ed07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input prompt in alpaca format\n",
    "\n",
    "prompt = \"\"\"### 지시:\n",
    "자료에서 느껴지는 감정이 긍정적인지 부정적인지 알려줘.\n",
    "\n",
    "### 자료:\n",
    "기분이 최고야!\n",
    "\n",
    "### 응답:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "62b0352a-f4a8-4304-a977-48e3c4a5e1a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### 지시:\n",
      "자료에서 느껴지는 감정이 긍정적인지 부정적인지 알려줘.\n",
      "\n",
      "### 자료:\n",
      "기분이 최고야!\n",
      "\n",
      "### 응답:\n",
      "긍정적인 감정입니다. 기분이 좋은 것으로 생각되어 있습니다. 어떤 감정이든 긍정적일 때가 많습니다. 긍정적인 감정입니다. \"그동안은 항상 즐거움으로 인해 매우 즐거운 일이 많았을 것 같아요.\"를 긍정적인 감정입니다. 그 중 하나는 긍정적인 감정입니다. 기쁨과 안도감, 충동, 만족감 등이 가장 큰 감정입니다. 기분이 좋은 곳으로 생각되던 경험을 통해, 그것은 긍정적인 경험으로 바꿀 수 있었다. 이 일을 하면 기분 좋은 순간입니다. 기분이 좋아\n"
     ]
    }
   ],
   "source": [
    "# Check generated response\n",
    "result = pipe(prompt)\n",
    "print(result[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b77d2822-af17-4655-bb29-06a478b9ed5a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
